{"timestamp":"2025-07-13T07:38:51.427819","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-13T07:38:51.428483","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bronze.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-13T07:38:51.525093","level":"info","event":"Connection Retrieved 'spark_submit_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-07-13T07:38:51.526043","level":"info","event":"Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.submit.deployMode=client --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --jars /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar,/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose --deploy-mode client /opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:51.585837","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.314475","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416474","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416740","level":"info","event":"master                  spark://spark-master:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416816","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416864","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416904","level":"info","event":"executorMemory          2g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416940","level":"info","event":"executorCores           2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.416976","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417011","level":"info","event":"propertiesFile          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417048","level":"info","event":"driverMemory            1g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417105","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417149","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417183","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417216","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417248","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417280","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417312","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417347","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417380","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417412","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417446","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417483","level":"info","event":"primaryResource         file:/opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417517","level":"info","event":"name                    minio_processing_job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417552","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417583","level":"info","event":"jars                    file:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar,file:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417619","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417652","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417683","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417715","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417750","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417786","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417818","level":"info","event":"--conf and those from the properties file null:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417851","level":"info","event":"(spark.driver.memory,1g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417883","level":"info","event":"(spark.executor.cores,2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417916","level":"info","event":"(spark.executor.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417947","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.417989","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.418026","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.731864","level":"info","event":"25/07/13 07:38:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.873554","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.873724","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.873796","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.873840","level":"info","event":"file:/opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.873879","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875657","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875766","level":"info","event":"(spark.app.name,minio_processing_job)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875818","level":"info","event":"(spark.app.submitTime,1752392333854)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875860","level":"info","event":"(spark.driver.memory,1g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875898","level":"info","event":"(spark.executor.cores,2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875935","level":"info","event":"(spark.executor.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.875970","level":"info","event":"(spark.jars,file:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar,file:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876005","level":"info","event":"(spark.master,spark://spark-master:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876039","level":"info","event":"(spark.repl.local.jars,file:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar,file:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876075","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876109","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876151","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876184","level":"info","event":"file:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876217","level":"info","event":"file:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876255","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.876292","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.878718","level":"info","event":"25/07/13 07:38:53 WARN DependencyUtils: Local jar /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar does not exist, skipping.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:53.878862","level":"info","event":"25/07/13 07:38:53 WARN DependencyUtils: Local jar /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar does not exist, skipping.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.384151","level":"info","event":"25/07/13 07:38:55 INFO SparkContext: Running Spark version 3.5.1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.384628","level":"info","event":"25/07/13 07:38:55 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.385087","level":"info","event":"25/07/13 07:38:55 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.408514","level":"info","event":"25/07/13 07:38:55 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.408948","level":"info","event":"25/07/13 07:38:55 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.409288","level":"info","event":"25/07/13 07:38:55 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.409703","level":"info","event":"25/07/13 07:38:55 INFO SparkContext: Submitted application: MinIOProcessingJob","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.425891","level":"info","event":"25/07/13 07:38:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.436010","level":"info","event":"25/07/13 07:38:55 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.437861","level":"info","event":"25/07/13 07:38:55 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.489778","level":"info","event":"25/07/13 07:38:55 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.491355","level":"info","event":"25/07/13 07:38:55 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.491910","level":"info","event":"25/07/13 07:38:55 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.492192","level":"info","event":"25/07/13 07:38:55 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.492599","level":"info","event":"25/07/13 07:38:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.713689","level":"info","event":"25/07/13 07:38:55 INFO Utils: Successfully started service 'sparkDriver' on port 36947.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.735781","level":"info","event":"25/07/13 07:38:55 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.768780","level":"info","event":"25/07/13 07:38:55 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.783960","level":"info","event":"25/07/13 07:38:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.784359","level":"info","event":"25/07/13 07:38:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.787602","level":"info","event":"25/07/13 07:38:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.806003","level":"info","event":"25/07/13 07:38:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aabc2b7a-8e7c-4313-a0ea-edd6a3adbaa0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.830393","level":"info","event":"25/07/13 07:38:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.842663","level":"info","event":"25/07/13 07:38:55 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:55.941368","level":"info","event":"25/07/13 07:38:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.021311","level":"info","event":"25/07/13 07:38:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063307","level":"info","event":"25/07/13 07:38:56 ERROR SparkContext: Failed to add file:/opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar to Spark environment","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063627","level":"info","event":"java.io.FileNotFoundException: Jar /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063708","level":"info","event":"at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063761","level":"info","event":"at org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063806","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063845","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063884","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063922","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063958","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.063995","level":"info","event":"at org.apache.spark.SparkContext.<init>(SparkContext.scala:526)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064031","level":"info","event":"at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064067","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064103","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064142","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064179","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064217","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064256","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064294","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064340","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:238)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064382","level":"info","event":"at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064420","level":"info","event":"at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064456","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064532","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064584","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064645","level":"info","event":"25/07/13 07:38:56 ERROR SparkContext: Failed to add file:/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar to Spark environment","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064688","level":"info","event":"java.io.FileNotFoundException: Jar /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064727","level":"info","event":"at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064763","level":"info","event":"at org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064804","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064878","level":"info","event":"at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.064966","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065036","level":"info","event":"at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065083","level":"info","event":"at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065127","level":"info","event":"at org.apache.spark.SparkContext.<init>(SparkContext.scala:526)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065165","level":"info","event":"at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065203","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065239","level":"info","event":"at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065285","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065324","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065370","level":"info","event":"at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065578","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065658","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065718","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:238)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065775","level":"info","event":"at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065823","level":"info","event":"at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065875","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065922","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.065981","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.134070","level":"info","event":"25/07/13 07:38:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.189414","level":"info","event":"25/07/13 07:38:56 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.5:7077 after 30 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.277783","level":"info","event":"25/07/13 07:38:56 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250713073856-0003","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.279259","level":"info","event":"25/07/13 07:38:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250713073856-0003/0 on worker-20250713072032-172.23.0.8-35049 (172.23.0.8:35049) with 2 core(s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.281304","level":"info","event":"25/07/13 07:38:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20250713073856-0003/0 on hostPort 172.23.0.8:35049 with 2 core(s), 2.0 GiB RAM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.284865","level":"info","event":"25/07/13 07:38:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34317.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.285020","level":"info","event":"25/07/13 07:38:56 INFO NettyBlockTransferService: Server created on 6be2e0f643cb:34317","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.287488","level":"info","event":"25/07/13 07:38:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.293217","level":"info","event":"25/07/13 07:38:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6be2e0f643cb, 34317, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.297181","level":"info","event":"25/07/13 07:38:56 INFO BlockManagerMasterEndpoint: Registering block manager 6be2e0f643cb:34317 with 434.4 MiB RAM, BlockManagerId(driver, 6be2e0f643cb, 34317, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.300438","level":"info","event":"25/07/13 07:38:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6be2e0f643cb, 34317, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.302049","level":"info","event":"25/07/13 07:38:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6be2e0f643cb, 34317, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.330111","level":"info","event":"25/07/13 07:38:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250713073856-0003/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.481629","level":"info","event":"25/07/13 07:38:56 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.650345","level":"info","event":"INFO:__main__:Spark session created successfully","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.650616","level":"info","event":"INFO:__main__:Reading data from s3a://input-bucket/input_data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.657711","level":"info","event":"25/07/13 07:38:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:56.659538","level":"info","event":"25/07/13 07:38:56 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.368939","level":"info","event":"25/07/13 07:38:57 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://input-bucket/input_data.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.369544","level":"info","event":"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.369668","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.369757","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.369836","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.369910","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.369986","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370061","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370290","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370406","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370500","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370583","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370660","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370736","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370810","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370882","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.370955","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371027","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371141","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371324","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371430","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371507","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371585","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371664","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371741","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371819","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371892","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.371966","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.372038","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.372108","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.372221","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.372328","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.372409","level":"info","event":"... 26 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429591","level":"info","event":"ERROR:__main__:Error in Spark job: An error occurred while calling o44.parquet.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429774","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429834","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429877","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429916","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429955","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.429993","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430028","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430062","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430096","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430130","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430164","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430198","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430232","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430265","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430297","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430333","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430367","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430400","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430431","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430465","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430498","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430531","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430563","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430594","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430626","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430658","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430689","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430721","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430752","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430782","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430813","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430847","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430879","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430913","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.430947","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.431014","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.431077","level":"info","event":"25/07/13 07:38:57 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.442506","level":"info","event":"25/07/13 07:38:57 INFO SparkUI: Stopped Spark web UI at http://6be2e0f643cb:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.446121","level":"info","event":"25/07/13 07:38:57 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.450110","level":"info","event":"25/07/13 07:38:57 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.465029","level":"info","event":"25/07/13 07:38:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.478441","level":"info","event":"25/07/13 07:38:57 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.478711","level":"info","event":"25/07/13 07:38:57 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.483333","level":"info","event":"25/07/13 07:38:57 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.486008","level":"info","event":"25/07/13 07:38:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.495283","level":"info","event":"25/07/13 07:38:57 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.509397","level":"info","event":"INFO:__main__:Spark session stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.509565","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.509627","level":"info","event":"File \"/opt/spark-jobs/spark_job.py\", line 64, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.514627","level":"info","event":"main()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.514765","level":"info","event":"File \"/opt/spark-jobs/spark_job.py\", line 45, in main","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.518864","level":"info","event":"df = spark.read.parquet(INPUT_PATH)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.519005","level":"info","event":"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.519065","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 544, in parquet","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.519299","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.519731","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.520046","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.520941","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o44.parquet.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521027","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521077","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521122","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521161","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521197","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521232","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521266","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521300","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521332","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521366","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521397","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521429","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521460","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521491","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521522","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521553","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521584","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521615","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521647","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521679","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521710","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521742","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521773","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521805","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521837","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521868","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521899","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521929","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521960","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.521991","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522021","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522051","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522082","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522112","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522143","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522178","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.522223","level":"info","event":"INFO:py4j.clientserver:Closing down clientserver connection","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.575086","level":"info","event":"25/07/13 07:38:57 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.575651","level":"info","event":"25/07/13 07:38:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bbbb0c1-45e1-42f9-ae76-c0b44a6853b4/pyspark-715b2fc0-a9da-498b-85d7-b98ad4e86a71","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.579313","level":"info","event":"25/07/13 07:38:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7cbe200-eaa6-4271-860e-7bb32b3a9ed4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.582595","level":"info","event":"25/07/13 07:38:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bbbb0c1-45e1-42f9-ae76-c0b44a6853b4","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:38:57.622672","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.submit.deployMode=client --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --jars /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar,/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose --deploy-mode client /opt/spark-jobs/spark_job.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":867,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1159,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
