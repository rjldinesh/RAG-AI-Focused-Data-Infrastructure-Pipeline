{"timestamp":"2025-07-13T04:24:19.940981","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-13T04:24:19.942112","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bronze.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-13T04:24:20.113784","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-07-13T04:24:20.116001","level":"info","event":"Spark-Submit cmd: spark-submit --master spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.submit.deployMode=client --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=ZPnglo5gVhmWx1iC0FdY --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose /opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:20.177365","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.544636","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658100","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658271","level":"info","event":"master                  spark-master:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658336","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658403","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658455","level":"info","event":"executorMemory          2g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658521","level":"info","event":"executorCores           2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658619","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658680","level":"info","event":"propertiesFile          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658725","level":"info","event":"driverMemory            1g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658764","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658802","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658838","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658873","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658908","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658943","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.658977","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659011","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659043","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659076","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659112","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659147","level":"info","event":"primaryResource         file:/opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659182","level":"info","event":"name                    minio_processing_job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659217","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659251","level":"info","event":"jars                    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659284","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659316","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659349","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659380","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659416","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659458","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659496","level":"info","event":"--conf and those from the properties file null:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659531","level":"info","event":"(spark.driver.memory,1g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659565","level":"info","event":"(spark.executor.cores,2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659601","level":"info","event":"(spark.executor.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659636","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659695","level":"info","event":"(spark.hadoop.fs.s3a.connection.ssl.enabled,false)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659745","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659785","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659823","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659859","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659895","level":"info","event":"(spark.master,spark://spark-master:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659930","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.659967","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.660005","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.873956","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.874124","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.874191","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.874234","level":"info","event":"file:/opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.874274","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876440","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876524","level":"info","event":"(spark.app.name,minio_processing_job)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876570","level":"info","event":"(spark.app.submitTime,1752380661855)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876610","level":"info","event":"(spark.driver.memory,1g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876646","level":"info","event":"(spark.executor.cores,2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876682","level":"info","event":"(spark.executor.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876718","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876753","level":"info","event":"(spark.hadoop.fs.s3a.connection.ssl.enabled,false)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876787","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876822","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876855","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876887","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876919","level":"info","event":"(spark.master,spark-master:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876952","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.876984","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.877024","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.877063","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.877099","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:21.877132","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.342511","level":"info","event":"25/07/13 04:24:23 INFO SparkContext: Running Spark version 3.5.1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.344491","level":"info","event":"25/07/13 04:24:23 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.344739","level":"info","event":"25/07/13 04:24:23 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.390244","level":"info","event":"25/07/13 04:24:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.462580","level":"info","event":"25/07/13 04:24:23 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.463016","level":"info","event":"25/07/13 04:24:23 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.463181","level":"info","event":"25/07/13 04:24:23 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.463604","level":"info","event":"25/07/13 04:24:23 INFO SparkContext: Submitted application: MinIOProcessingJob","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.480873","level":"info","event":"25/07/13 04:24:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.488751","level":"info","event":"25/07/13 04:24:23 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.490756","level":"info","event":"25/07/13 04:24:23 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.536852","level":"info","event":"25/07/13 04:24:23 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.537292","level":"info","event":"25/07/13 04:24:23 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.537641","level":"info","event":"25/07/13 04:24:23 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.538010","level":"info","event":"25/07/13 04:24:23 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.538629","level":"info","event":"25/07/13 04:24:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.777089","level":"info","event":"25/07/13 04:24:23 INFO Utils: Successfully started service 'sparkDriver' on port 33707.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.805081","level":"info","event":"25/07/13 04:24:23 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.835137","level":"info","event":"25/07/13 04:24:23 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.850896","level":"info","event":"25/07/13 04:24:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.851568","level":"info","event":"25/07/13 04:24:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.855034","level":"info","event":"25/07/13 04:24:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.874646","level":"info","event":"25/07/13 04:24:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a200ffac-b6fa-4886-90ff-e5d63c79b955","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.888921","level":"info","event":"25/07/13 04:24:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:23.904664","level":"info","event":"25/07/13 04:24:23 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:24.028271","level":"info","event":"25/07/13 04:24:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:24.080274","level":"info","event":"25/07/13 04:24:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:24.214372","level":"info","event":"25/07/13 04:24:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:24.273045","level":"info","event":"25/07/13 04:24:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.6:7077 after 34 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.098254","level":"info","event":"25/07/13 04:24:25 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250713042425-0000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.110438","level":"info","event":"25/07/13 04:24:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38835.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.110700","level":"info","event":"25/07/13 04:24:25 INFO NettyBlockTransferService: Server created on 4af69c652026:38835","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.115864","level":"info","event":"25/07/13 04:24:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.124725","level":"info","event":"25/07/13 04:24:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4af69c652026, 38835, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.128972","level":"info","event":"25/07/13 04:24:25 INFO BlockManagerMasterEndpoint: Registering block manager 4af69c652026:38835 with 434.4 MiB RAM, BlockManagerId(driver, 4af69c652026, 38835, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.134168","level":"info","event":"25/07/13 04:24:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4af69c652026, 38835, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.135200","level":"info","event":"25/07/13 04:24:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250713042425-0000/0 on worker-20250713032310-172.21.0.8-43037 (172.21.0.8:43037) with 2 core(s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.135592","level":"info","event":"25/07/13 04:24:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4af69c652026, 38835, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.138372","level":"info","event":"25/07/13 04:24:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250713042425-0000/0 on hostPort 172.21.0.8:43037 with 2 core(s), 2.0 GiB RAM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.408812","level":"info","event":"25/07/13 04:24:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.723885","level":"info","event":"INFO:__main__:Spark session created successfully","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.724196","level":"info","event":"INFO:__main__:Reading data from s3a://input-bucket/input_data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.836740","level":"info","event":"25/07/13 04:24:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:25.848338","level":"info","event":"25/07/13 04:24:25 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:26.336444","level":"info","event":"25/07/13 04:24:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250713042425-0000/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341348","level":"info","event":"25/07/13 04:24:27 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://input-bucket/input_data.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341529","level":"info","event":"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341594","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341641","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341684","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341723","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341764","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341811","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341857","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341895","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341931","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.341968","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342192","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342264","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342308","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342348","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342386","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342423","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342459","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342495","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342533","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342572","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342613","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342743","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342795","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342837","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342900","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.342955","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.343003","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.343043","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.343400","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.343500","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.343545","level":"info","event":"... 26 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398205","level":"info","event":"ERROR:__main__:Error in Spark job: An error occurred while calling o40.parquet.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398374","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398440","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398507","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398557","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398612","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398679","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398718","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398754","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398793","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398831","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398867","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398902","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398939","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.398976","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399011","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399047","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399084","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399120","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399157","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399192","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399226","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399262","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399299","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399337","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399372","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399408","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399444","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399478","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399512","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399548","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399583","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399617","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399652","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399687","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399722","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399760","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.399858","level":"info","event":"25/07/13 04:24:27 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.412209","level":"info","event":"25/07/13 04:24:27 INFO SparkUI: Stopped Spark web UI at http://4af69c652026:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.418338","level":"info","event":"25/07/13 04:24:27 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.422597","level":"info","event":"25/07/13 04:24:27 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.438565","level":"info","event":"25/07/13 04:24:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.466638","level":"info","event":"25/07/13 04:24:27 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.467585","level":"info","event":"25/07/13 04:24:27 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.478412","level":"info","event":"25/07/13 04:24:27 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.481805","level":"info","event":"25/07/13 04:24:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.501403","level":"info","event":"25/07/13 04:24:27 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.945328","level":"info","event":"INFO:__main__:Spark session stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.945743","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.945898","level":"info","event":"File \"/opt/spark-jobs/spark_job.py\", line 63, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.952700","level":"info","event":"main()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.952994","level":"info","event":"File \"/opt/spark-jobs/spark_job.py\", line 44, in main","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.960068","level":"info","event":"df = spark.read.parquet(INPUT_PATH)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.960247","level":"info","event":"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.960308","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 544, in parquet","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.962252","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.962812","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.963216","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965231","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o40.parquet.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965472","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965549","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965598","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965643","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965690","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965733","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965775","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965815","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965859","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965898","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965940","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.965984","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966030","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966076","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966114","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966151","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966191","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966232","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966282","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966328","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966377","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966425","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966521","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966588","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966648","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966698","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966750","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966803","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966864","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966923","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.966968","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.967012","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.967063","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.967109","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.967154","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.967216","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:27.967355","level":"info","event":"INFO:py4j.clientserver:Closing down clientserver connection","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:28.045124","level":"info","event":"25/07/13 04:24:28 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:28.047082","level":"info","event":"25/07/13 04:24:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-5bc5caa9-54c7-4ba5-8cba-153b6d726284","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:28.056308","level":"info","event":"25/07/13 04:24:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-5bc5caa9-54c7-4ba5-8cba-153b6d726284/pyspark-317adf0d-3a08-4ec3-b861-e9069e135e09","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:28.063188","level":"info","event":"25/07/13 04:24:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-9df4e1b7-a9bc-44a4-8ba4-1893b4833f47","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T04:24:28.099773","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.submit.deployMode=client --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=ZPnglo5gVhmWx1iC0FdY --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose /opt/spark-jobs/spark_job.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":867,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1159,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
