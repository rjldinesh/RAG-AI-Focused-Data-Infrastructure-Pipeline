{"timestamp":"2025-07-13T06:41:19.645861","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-13T06:41:19.646508","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bronze.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-13T06:41:19.811231","level":"error","event":"Unable to retrieve connection from secrets backend (EnvironmentVariablesBackend). Checking subsequent secrets backend.","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Invalid connection string: spark://spark://spark-master:7077.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/context.py","lineno":130,"name":"_get_connection"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/secrets/base_secrets.py","lineno":79,"name":"get_connection"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/secrets/base_secrets.py","lineno":66,"name":"deserialize_connection"},{"filename":"<string>","lineno":4,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/state.py","lineno":481,"name":"_initialize_instance"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py","lineno":70,"name":"__exit__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py","lineno":211,"name":"raise_"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/state.py","lineno":479,"name":"_initialize_instance"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/connection.py","lineno":163,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/connection.py","lineno":222,"name":"_parse_from_uri"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-07-13T06:41:20.335956","level":"info","event":"Could not load connection string spark_default_master, defaulting to yarn","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:20.336673","level":"info","event":"Spark-Submit cmd: spark-submit --master yarn --conf spark.submit.deployMode=client --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose /opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:20.386232","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.017427","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.136684","level":"info","event":"Exception in thread \"main\" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.136979","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137083","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137155","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137220","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137284","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1103)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137345","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1103)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137409","level":"info","event":"at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137467","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137526","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.137587","level":"info","event":"at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T06:41:22.159957","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master yarn --conf spark.submit.deployMode=client --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose /opt/spark-jobs/spark_job.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":867,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1159,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
