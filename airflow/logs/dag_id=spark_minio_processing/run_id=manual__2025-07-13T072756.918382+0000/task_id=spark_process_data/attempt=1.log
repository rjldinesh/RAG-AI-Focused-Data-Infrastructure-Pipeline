{"timestamp":"2025-07-13T07:27:57.571973","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-13T07:27:57.572578","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/bronze.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-13T07:27:57.638377","level":"info","event":"Connection Retrieved 'spark_submit_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-07-13T07:27:57.639149","level":"info","event":"Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.submit.deployMode=client --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose --deploy-mode client /opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:57.688829","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.267657","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414435","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414600","level":"info","event":"master                  spark://spark-master:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414665","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414711","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414752","level":"info","event":"executorMemory          2g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414790","level":"info","event":"executorCores           2","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414827","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414863","level":"info","event":"propertiesFile          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414898","level":"info","event":"driverMemory            1g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414932","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414966","level":"info","event":"driverExtraClassPath    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.414999","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415032","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415066","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415098","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415131","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415165","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415229","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415289","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415329","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415365","level":"info","event":"primaryResource         file:/opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415399","level":"info","event":"name                    minio_processing_job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415436","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415469","level":"info","event":"jars                    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415502","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415535","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415568","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415599","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415633","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415669","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415703","level":"info","event":"--conf and those from the properties file null:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415735","level":"info","event":"(spark.driver.memory,1g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415765","level":"info","event":"(spark.executor.cores,2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415797","level":"info","event":"(spark.executor.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415828","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415860","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.415893","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.655090","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.655345","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.655428","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.655473","level":"info","event":"file:/opt/spark-jobs/spark_job.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.655511","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657598","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657700","level":"info","event":"(spark.app.name,minio_processing_job)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657751","level":"info","event":"(spark.app.submitTime,1752391679631)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657795","level":"info","event":"(spark.driver.memory,1g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657833","level":"info","event":"(spark.executor.cores,2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657871","level":"info","event":"(spark.executor.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657907","level":"info","event":"(spark.master,spark://spark-master:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657944","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.657979","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.658020","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.658058","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.658154","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:27:59.658204","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.139091","level":"info","event":"25/07/13 07:28:01 INFO SparkContext: Running Spark version 3.5.1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.141448","level":"info","event":"25/07/13 07:28:01 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.141824","level":"info","event":"25/07/13 07:28:01 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.194636","level":"info","event":"25/07/13 07:28:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.268882","level":"info","event":"25/07/13 07:28:01 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.269144","level":"info","event":"25/07/13 07:28:01 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.269384","level":"info","event":"25/07/13 07:28:01 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.269674","level":"info","event":"25/07/13 07:28:01 INFO SparkContext: Submitted application: MinIOProcessingJob","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.289425","level":"info","event":"25/07/13 07:28:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.299298","level":"info","event":"25/07/13 07:28:01 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.301185","level":"info","event":"25/07/13 07:28:01 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.357354","level":"info","event":"25/07/13 07:28:01 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.357698","level":"info","event":"25/07/13 07:28:01 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.358353","level":"info","event":"25/07/13 07:28:01 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.358816","level":"info","event":"25/07/13 07:28:01 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.359302","level":"info","event":"25/07/13 07:28:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.618748","level":"info","event":"25/07/13 07:28:01 INFO Utils: Successfully started service 'sparkDriver' on port 45643.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.654453","level":"info","event":"25/07/13 07:28:01 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.693988","level":"info","event":"25/07/13 07:28:01 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.711564","level":"info","event":"25/07/13 07:28:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.712391","level":"info","event":"25/07/13 07:28:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.716267","level":"info","event":"25/07/13 07:28:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.739692","level":"info","event":"25/07/13 07:28:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-92b69e3f-1b80-4129-99fa-ae308267925b","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.752801","level":"info","event":"25/07/13 07:28:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.768655","level":"info","event":"25/07/13 07:28:01 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.885531","level":"info","event":"25/07/13 07:28:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:01.938752","level":"info","event":"25/07/13 07:28:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.081434","level":"info","event":"25/07/13 07:28:02 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.131192","level":"info","event":"25/07/13 07:28:02 INFO TransportClientFactory: Successfully created connection to spark-master/172.23.0.5:7077 after 29 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.227423","level":"info","event":"25/07/13 07:28:02 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250713072802-0001","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.233170","level":"info","event":"25/07/13 07:28:02 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250713072802-0001/0 on worker-20250713072032-172.23.0.8-35049 (172.23.0.8:35049) with 2 core(s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.235822","level":"info","event":"25/07/13 07:28:02 INFO StandaloneSchedulerBackend: Granted executor ID app-20250713072802-0001/0 on hostPort 172.23.0.8:35049 with 2 core(s), 2.0 GiB RAM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.245831","level":"info","event":"25/07/13 07:28:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32901.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.246084","level":"info","event":"25/07/13 07:28:02 INFO NettyBlockTransferService: Server created on 6be2e0f643cb:32901","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.249227","level":"info","event":"25/07/13 07:28:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.259495","level":"info","event":"25/07/13 07:28:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6be2e0f643cb, 32901, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.264841","level":"info","event":"25/07/13 07:28:02 INFO BlockManagerMasterEndpoint: Registering block manager 6be2e0f643cb:32901 with 434.4 MiB RAM, BlockManagerId(driver, 6be2e0f643cb, 32901, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.268543","level":"info","event":"25/07/13 07:28:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6be2e0f643cb, 32901, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.270054","level":"info","event":"25/07/13 07:28:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6be2e0f643cb, 32901, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.305232","level":"info","event":"25/07/13 07:28:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250713072802-0001/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.467217","level":"info","event":"25/07/13 07:28:02 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.696264","level":"info","event":"INFO:__main__:Spark session created successfully","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.696463","level":"info","event":"INFO:__main__:Reading data from s3a://input-bucket/input_data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.766312","level":"info","event":"25/07/13 07:28:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:02.776743","level":"info","event":"25/07/13 07:28:02 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.733942","level":"info","event":"25/07/13 07:28:03 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://input-bucket/input_data.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734293","level":"info","event":"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734385","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734434","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734477","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734516","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734553","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734591","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734627","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734662","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734699","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734737","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734773","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734813","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734850","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734886","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734921","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734956","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.734993","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735029","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735067","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735104","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735140","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735176","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735210","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735245","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735299","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735356","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735397","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735437","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735476","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735513","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.735550","level":"info","event":"... 26 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.800952","level":"info","event":"ERROR:__main__:Error in Spark job: An error occurred while calling o42.parquet.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801281","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801373","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801436","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801494","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801547","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801598","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801643","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801689","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801738","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801799","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801856","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801912","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.801967","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802018","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802072","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802120","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802173","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802222","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802273","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802321","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802414","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802482","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802536","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802586","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802655","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802703","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802751","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802798","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.802995","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803092","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803147","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803196","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803245","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803291","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803338","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803386","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.803438","level":"info","event":"25/07/13 07:28:03 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.818922","level":"info","event":"25/07/13 07:28:03 INFO SparkUI: Stopped Spark web UI at http://6be2e0f643cb:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.823337","level":"info","event":"25/07/13 07:28:03 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.826510","level":"info","event":"25/07/13 07:28:03 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.844847","level":"info","event":"25/07/13 07:28:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.859182","level":"info","event":"25/07/13 07:28:03 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.859785","level":"info","event":"25/07/13 07:28:03 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.866396","level":"info","event":"25/07/13 07:28:03 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.868175","level":"info","event":"25/07/13 07:28:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.882682","level":"info","event":"25/07/13 07:28:03 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:03.999714","level":"info","event":"INFO:__main__:Spark session stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.000029","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.000152","level":"info","event":"File \"/opt/spark-jobs/spark_job.py\", line 64, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.009434","level":"info","event":"main()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.009769","level":"info","event":"File \"/opt/spark-jobs/spark_job.py\", line 45, in main","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.016943","level":"info","event":"df = spark.read.parquet(INPUT_PATH)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.017246","level":"info","event":"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.017376","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 544, in parquet","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.017827","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.018944","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.019715","level":"info","event":"File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.021672","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o42.parquet.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.021831","level":"info","event":": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.021921","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022048","level":"info","event":"at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022135","level":"info","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022214","level":"info","event":"at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022286","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022354","level":"info","event":"at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022436","level":"info","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022522","level":"info","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022606","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022747","level":"info","event":"at scala.collection.immutable.List.map(List.scala:293)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.022988","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023116","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023245","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023384","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023487","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023593","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023688","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023774","level":"info","event":"at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023865","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.023951","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024036","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024121","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024202","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024283","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024362","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024439","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024516","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024594","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024671","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024748","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024825","level":"info","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024902","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.024979","level":"info","event":"at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.025055","level":"info","event":"... 29 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.025140","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.025300","level":"info","event":"INFO:py4j.clientserver:Closing down clientserver connection","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.111614","level":"info","event":"25/07/13 07:28:04 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.113544","level":"info","event":"25/07/13 07:28:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-702a8a64-1c8a-49c0-b7fc-a41fdbcdd70d","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.118433","level":"info","event":"25/07/13 07:28:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-946d9c17-663a-475e-8b2d-7dbd76bca598/pyspark-1aaf8e8f-ab97-4417-a8ee-58cdbe8071ae","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.126025","level":"info","event":"25/07/13 07:28:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-946d9c17-663a-475e-8b2d-7dbd76bca598","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-07-13T07:28:04.190795","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.submit.deployMode=client --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name minio_processing_job --verbose --deploy-mode client /opt/spark-jobs/spark_job.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":867,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1159,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
