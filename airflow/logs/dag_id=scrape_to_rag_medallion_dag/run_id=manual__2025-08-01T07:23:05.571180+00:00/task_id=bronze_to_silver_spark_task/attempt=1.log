{"timestamp":"2025-08-01T07:23:26.864353","level":"warning","event":"\n        OpenLineage support for Airflow version 3.0.2 is REMOVED.\n        For Airflow 2.7 and later, use the native Airflow Openlineage provider package.\n        Documentation can be found at https://airflow.apache.org/docs/apache-airflow-providers-openlineage\n        ","logger":"root"}
{"timestamp":"2025-08-01T07:23:26.947127","level":"warning","event":"No module named 'airflow.providers.dbt'","logger":"openlineage.airflow.utils"}
{"timestamp":"2025-08-01T07:23:26.949127","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-08-01T07:23:26.949335","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/activefence.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-08-01T07:23:27.404305","level":"info","event":"Could not load connection string spark_submit_default, defaulting to yarn","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:27.406110","level":"info","event":"Spark-Submit cmd: /opt/spark/bin/spark-submit --master yarn --conf spark.submit.deployMode=client --conf spark.hadoop.fs.s3a.endpoint=http://project-v2-minio-1:9000 --conf spark.hadoop.fs.s3a.access.key=ZPnglo5gVhmWx1iC0FdY --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --conf spark.yarn.appMasterEnv.JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64 --conf spark.yarn.appMasterEnv.PATH=/usr/lib/jvm/java-17-openjdk-arm64/bin:/opt/spark/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin:/opt/spark/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin:/opt/spark/bin:/root/bin:/home/airflow/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.520,org.apache.spark:spark-avro_2.12:3.5.0,io.delta:delta-spark_2.12:3.1.0 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name bronze_to_silver_job --verbose /opt/spark-jobs/bronze_to_silver.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.252614","level":"info","event":"Using properties file: null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294516","level":"info","event":"Exception in thread \"main\" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294617","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294645","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294668","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294688","level":"info","event":"at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294705","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1103)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294723","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1103)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294741","level":"info","event":"at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294759","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294776","level":"info","event":"at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.294794","level":"info","event":"at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-08-01T07:23:28.302539","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: /opt/spark/bin/spark-submit --master yarn --conf spark.submit.deployMode=client --conf spark.hadoop.fs.s3a.endpoint=http://project-v2-minio-1:9000 --conf spark.hadoop.fs.s3a.access.key=ZPnglo5gVhmWx1iC0FdY --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.executor.memory=2g --conf spark.driver.memory=1g --conf spark.executor.cores=2 --conf spark.yarn.appMasterEnv.JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64 --conf spark.yarn.appMasterEnv.PATH=/usr/lib/jvm/java-17-openjdk-arm64/bin:/opt/spark/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin:/opt/spark/bin:/usr/lib/jvm/java-17-openjdk-amd64/bin:/opt/spark/bin:/root/bin:/home/airflow/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.520,org.apache.spark:spark-avro_2.12:3.5.0,io.delta:delta-spark_2.12:3.1.0 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name bronze_to_silver_job --verbose /opt/spark-jobs/bronze_to_silver.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":867,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1159,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":570,"name":"submit"}],"is_group":false,"exceptions":[]}]}
