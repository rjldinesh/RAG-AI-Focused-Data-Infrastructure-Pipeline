{"timestamp":"2025-07-13T07:47:39.478774","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-13T07:47:39.479783","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/main.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-13T07:47:39.933178Z","level":"error","event":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:41.645104Z","level":"error","event":"25/07/13 07:47:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:41.795965Z","level":"error","event":"25/07/13 07:47:41 WARN DependencyUtils: Local jar /opt/bitnami/spark/jars/hadoop-aws-3.3.5.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:41.796992Z","level":"error","event":"25/07/13 07:47:41 WARN DependencyUtils: Local jar /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.520.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.028232Z","level":"error","event":"25/07/13 07:47:42 INFO SparkContext: Running Spark version 3.5.1","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.029159Z","level":"error","event":"25/07/13 07:47:42 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.029615Z","level":"error","event":"25/07/13 07:47:42 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.060848Z","level":"error","event":"25/07/13 07:47:42 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.061598Z","level":"error","event":"25/07/13 07:47:42 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.062070Z","level":"error","event":"25/07/13 07:47:42 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.062680Z","level":"error","event":"25/07/13 07:47:42 INFO SparkContext: Submitted application: BBC Tech Pipeline","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.077833Z","level":"error","event":"25/07/13 07:47:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.084730Z","level":"error","event":"25/07/13 07:47:42 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.085420Z","level":"error","event":"25/07/13 07:47:42 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.126517Z","level":"error","event":"25/07/13 07:47:42 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.127190Z","level":"error","event":"25/07/13 07:47:42 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.127760Z","level":"error","event":"25/07/13 07:47:42 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.128289Z","level":"error","event":"25/07/13 07:47:42 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.128985Z","level":"error","event":"25/07/13 07:47:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.317358Z","level":"error","event":"25/07/13 07:47:42 INFO Utils: Successfully started service 'sparkDriver' on port 39885.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.342415Z","level":"error","event":"25/07/13 07:47:42 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.377507Z","level":"error","event":"25/07/13 07:47:42 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.393897Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.397497Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.411497Z","level":"error","event":"25/07/13 07:47:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.434305Z","level":"error","event":"25/07/13 07:47:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ac46ac3-7f7b-46a4-ac0e-df62508a3e63","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.452271Z","level":"error","event":"25/07/13 07:47:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.476531Z","level":"error","event":"25/07/13 07:47:42 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.620018Z","level":"error","event":"25/07/13 07:47:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.706688Z","level":"error","event":"25/07/13 07:47:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.744014Z","level":"error","event":"25/07/13 07:47:42 ERROR SparkContext: Failed to add /opt/bitnami/spark/jars/hadoop-aws-3.3.5.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.744864Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/bitnami/spark/jars/hadoop-aws-3.3.5.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.745486Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.745930Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.746417Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.746839Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.747243Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.747604Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.748088Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.748539Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.749021Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.749452Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.749896Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.750337Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.750939Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.751610Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.752187Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.752527Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.752903Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.753185Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.753448Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.753710Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.753968Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.754279Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.754621Z","level":"error","event":"25/07/13 07:47:42 ERROR SparkContext: Failed to add /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.520.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.754921Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.520.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.755258Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.755672Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.756030Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.756397Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.756756Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.757150Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.757807Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.758311Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.758668Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.758962Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.759246Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.759587Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.759891Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.760256Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.760580Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.760828Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.761286Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.761560Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.761845Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.762134Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.762502Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.762990Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.799307Z","level":"error","event":"25/07/13 07:47:42 INFO Executor: Starting executor ID driver on host 6be2e0f643cb","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.800670Z","level":"error","event":"25/07/13 07:47:42 INFO Executor: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.801411Z","level":"error","event":"25/07/13 07:47:42 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.807425Z","level":"error","event":"25/07/13 07:47:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.808123Z","level":"error","event":"25/07/13 07:47:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5a75847c for default.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.828125Z","level":"error","event":"25/07/13 07:47:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42363.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.829173Z","level":"error","event":"25/07/13 07:47:42 INFO NettyBlockTransferService: Server created on 6be2e0f643cb:42363","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.830618Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.837742Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6be2e0f643cb, 42363, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.840505Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManagerMasterEndpoint: Registering block manager 6be2e0f643cb:42363 with 434.4 MiB RAM, BlockManagerId(driver, 6be2e0f643cb, 42363, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.842445Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6be2e0f643cb, 42363, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:42.843330Z","level":"error","event":"25/07/13 07:47:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6be2e0f643cb, 42363, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.212903Z","level":"error","event":"25/07/13 07:47:43 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.214171Z","level":"error","event":"java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.215000Z","level":"error","event":"\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.215849Z","level":"error","event":"\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.216382Z","level":"error","event":"\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.217022Z","level":"error","event":"\tat java.base/java.lang.Class.forName0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.217487Z","level":"error","event":"\tat java.base/java.lang.Class.forName(Class.java:467)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.218053Z","level":"error","event":"\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.218490Z","level":"error","event":"\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.218952Z","level":"error","event":"\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.219454Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.220030Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.220471Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.220858Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.221461Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.221869Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.222312Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.222729Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.223131Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.223687Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.223997Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.224268Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.224555Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.224818Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.225217Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.225568Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.225997Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.226385Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.226874Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.227156Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.227903Z","level":"error","event":"25/07/13 07:47:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.229898Z","level":"error","event":"25/07/13 07:47:43 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.899811Z","level":"error","event":"25/07/13 07:47:43 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://activefence-bucket/bbc_tech/land/*.html.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.900842Z","level":"error","event":"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.901862Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.902432Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.903104Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.903601Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.904348Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.905068Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.905715Z","level":"error","event":"\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.906252Z","level":"error","event":"\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.906790Z","level":"error","event":"\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.907080Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.907691Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.908344Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.908875Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.909366Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.909775Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.910139Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.910749Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.911221Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.911702Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.912128Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.912589Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.913069Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.913503Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.913951Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.914398Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.914834Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.915267Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.915682Z","level":"error","event":"Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.916136Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.916537Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.917134Z","level":"error","event":"\t... 26 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.943225","level":"error","event":"Bronze processing failed: An error occurred while calling o47.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n","logger":"unusual_prefix_8f3fa82684731fec338e7a38d6fbf711ac450beb_main"}
{"timestamp":"2025-07-13T07:47:43.944275Z","level":"error","event":"25/07/13 07:47:43 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.955084Z","level":"error","event":"25/07/13 07:47:43 INFO SparkUI: Stopped Spark web UI at http://6be2e0f643cb:4040","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.970940Z","level":"error","event":"25/07/13 07:47:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.982727Z","level":"error","event":"25/07/13 07:47:43 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.983686Z","level":"error","event":"25/07/13 07:47:43 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.987586Z","level":"error","event":"25/07/13 07:47:43 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:43.990722Z","level":"error","event":"25/07/13 07:47:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:44.003000Z","level":"error","event":"25/07/13 07:47:44 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:44.053718","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o47.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":867,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1159,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/main.py","lineno":136,"name":"process_bronze"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":307,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-07-13T07:47:44.075638Z","level":"error","event":"25/07/13 07:47:44 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:44.076963Z","level":"error","event":"25/07/13 07:47:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bd260d2-512c-4b19-aef1-0f118f697054","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:44.083273Z","level":"error","event":"25/07/13 07:47:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bd260d2-512c-4b19-aef1-0f118f697054/pyspark-0484a7a2-ee63-4516-8a82-2f9d32dea99a","chan":"stderr","logger":"task"}
{"timestamp":"2025-07-13T07:47:44.088745Z","level":"error","event":"25/07/13 07:47:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-4d152407-67f0-456c-a214-be8155fc4ec0","chan":"stderr","logger":"task"}
